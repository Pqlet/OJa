{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a28b2c1-ae69-49c8-9c83-cc384894f0d5",
   "metadata": {},
   "source": [
    "# Creating a dataset class with activations from the hooks and corresponding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28f3c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformer_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98bc3abd-c0d6-4f8e-b618-ddbb101d5b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from tqdm import tqdm\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, IterableDataset, load_dataset\n",
    "from numpy.typing import NDArray\n",
    "from pydantic import BaseModel, ConfigDict\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from transformer_lens import HookedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "285de65a-9ff7-44c4-981d-f15a08cce11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetConfig(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\", frozen=True)\n",
    "    dataset_name: str\n",
    "    is_tokenized: bool = True\n",
    "    tokenizer_name: str\n",
    "    streaming: bool = True\n",
    "    split: str\n",
    "    n_ctx: int\n",
    "    seed: int = 0\n",
    "    column_name: str = \"input_ids\"\n",
    "    \"\"\"The name of the column in the dataset that contains the data (tokenized or non-tokenized).\n",
    "    Typically 'input_ids' for datasets stored with e2e_sae/scripts/upload_hf_dataset.py, or \"tokens\"\n",
    "    for datasets tokenized in TransformerLens (e.g. NeelNanda/pile-10k).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ec43a0c-73a1-41c6-a8aa-f3beccb655a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config  = DatasetConfig\n",
    "\n",
    "dataset_config.model_config = None # blank\n",
    "dataset_config.dataset_name= 'roneneldan/TinyStories' # 'apollo-research/roneneldan-TinyStories-tokenizer-gpt2'\n",
    "dataset_config.is_tokenized= False # True\n",
    "dataset_config.tokenizer_name= 'gpt2'\n",
    "dataset_config.streaming= True # True - means you do not download the dataset https://huggingface.co/docs/datasets/en/stream\n",
    "dataset_config.split= 'train' # ['train', 'validation']\n",
    "dataset_config.n_ctx= 512\n",
    "dataset_config.seed= 0\n",
    "dataset_config.column_name: str = \"input_ids\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e197b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' # 'cpu', 'cuda'\n",
    "model = HookedTransformer.from_pretrained(\"tiny-stories-1M\").to(device)\n",
    "dataset = load_dataset(\n",
    "    dataset_config.dataset_name, streaming=dataset_config.streaming, split=dataset_config.split\n",
    ")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(dataset_config.tokenizer_name) # 'gpt2' for tiny-stories-1M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46f3ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gather hidden representations of the residual stream after the block  blocks.5\n",
    "    blocks.5.hook_resid_post\n",
    "\"\"\"\n",
    "\n",
    "n_seqs = 3200 # 3200 # Num sequences to process\n",
    "embs_list = []\n",
    "tokens_list = []\n",
    "text_id_list = []\n",
    "seqs_saved = []\n",
    "for i, v in tqdm(enumerate(dataset), total=n_seqs):\n",
    "    text_ = v['text']\n",
    "    tokenized = model.tokenizer(text_)['input_ids'] # No BOS token\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits_, cache_ = model.run_with_cache(text_)\n",
    "    acts_ = cache_['blocks.5.hook_resid_post'][0].cpu().numpy()[1:] # Skip BOS token\n",
    "    # Append to lists\n",
    "    embs_list.append(acts_)\n",
    "    tokens_list.append(tokenized)\n",
    "    text_id_list = text_id_list + [i]*len(acts_)\n",
    "    seqs_saved.append(text_)\n",
    "    if i >= n_seqs:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba228b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "embs_flat = np.array([v for vv in embs_list for v in vv])\n",
    "tokens_str_list_flat = [model.tokenizer.decode(v) for vv in tokens_list for v in vv]\n",
    "assert (len(embs_flat)==len(tokens_str_list_flat)==len(text_id_list)), 'Not equal len'\n",
    "\n",
    "# Save embedding matrix\n",
    "print(f\"Embedding matrix takes {embs_flat.nbytes/1024**2:.3f} Mb\")\n",
    "datadir = Path ('./')\n",
    "filepath_acts = datadir / f\"emb_matrix.npy\"\n",
    "with open(filepath_acts, 'wb') as f:\n",
    "    np.save(f, embs_flat)\n",
    "\n",
    "df_acts_tokens = pd.DataFrame()\n",
    "df_acts_tokens['emb_id'] = list(range(len(embs_flat)))\n",
    "df_acts_tokens['token_str'] = tokens_str_list_flat\n",
    "df_acts_tokens['seq_id'] = text_id_list\n",
    "df_acts_tokens.emb_id = df_acts_tokens.emb_id.astype(np.int32)\n",
    "df_acts_tokens.seq_id = df_acts_tokens.seq_id.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07e015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acts_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "093df139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play with sorting tokens by neuron activations\n",
    "# qq = np.array(acts_list_flat)\n",
    "# df_acts_tokens['nn'] = qq[:, 1]\n",
    "# df_acts_tokens.sort_values(by='nn', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "203d1632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset of texts activations have been acquired from\n",
    "df_texts = pd.DataFrame()\n",
    "df_texts['seq'] = seqs_saved\n",
    "df_texts['seq_id'] = df_texts.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10cc0cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Left join activations dataframe with sequence dataframe\n",
    "# df_acts_tokens.merge(df_texts, how='left', on='seq_id').info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bbb3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acts_tokens.to_csv(datadir / \"tinystories1M-TinyStories1_gpt2token-acts_id.csv\", index=False)\n",
    "df_texts.to_csv(datadir / \"tinystories1M-TinyStories1_gpt2token-texts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac238f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_acts_tokens\n",
    "del df_texts\n",
    "del embs_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "101f81b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# datadir = Path(\"./\")\n",
    "\n",
    "# # Load DataFrames with embedding ids and texts\n",
    "# df_acts_filename = datadir / \"tinystories1M-TinyStories1_gpt2token-acts_id.csv\"\n",
    "# df_texts_filename = datadir / \"tinystories1M-TinyStories1_gpt2token-texts.csv\"\n",
    "# df_acts_tokens = pd.read_csv(df_acts_filename)\n",
    "# df_texts= pd.read_csv(df_texts_filename)\n",
    "\n",
    "# # Load embedding matrix\n",
    "# with open(datadir / 'emb_matrix.npy', 'rb') as f:\n",
    "#     embs_flat = np.load(f)\n",
    "    \n",
    "# # # Left join activations dataframe with sequence dataframe\n",
    "# # df_acts_tokens_texts = df_acts_tokens.merge(df_texts, how='left', on='seq_id').info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmpttv-jup",
   "language": "python",
   "name": "cmpttv-jup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
