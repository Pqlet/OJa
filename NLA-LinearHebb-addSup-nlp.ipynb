{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Demo single-file notebook to train a ConvNet on CIFAR10 using SoftHebb, an unsupervised, efficient and bio-plausible learning algorithm.\n",
    "Based on demo.py from the official repo\n",
    "\"\"\"\n",
    "import math\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.utils import _pair\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchvision\n",
    "\n",
    "def seed_init_fn(seed):\n",
    "    seed = seed % 2 ** 32\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Lateral Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_circular_mask_batched(h, w, centers, radii):\n",
    "    \"\"\"\n",
    "    Create circular masks for batched inputs using PyTorch tensors.\n",
    "    \n",
    "    Args:\n",
    "    - h: Height of the grid (or a tensor of heights for each batch).\n",
    "    - w: Width of the grid (or a tensor of widths for each batch).\n",
    "    - centers: A tensor of shape (batch_size, 2) representing the (y, x) center of the circle for each batch.\n",
    "               If None, defaults to the center of each grid.\n",
    "    - radii: A tensor of shape (batch_size,) representing the radius of the circle for each batch.\n",
    "             If None, defaults to the minimum distance from the center to the edges of each grid.\n",
    "    \n",
    "    Returns:\n",
    "    - A boolean mask (tensor) of shape (batch_size, h, w), where True represents points inside the circle.\n",
    "    \"\"\"\n",
    "    batch_size = centers.size(0)\n",
    "\n",
    "    # Ensure `h` and `w` are tensors if they are scalar inputs\n",
    "    if isinstance(h, int):\n",
    "        h = torch.tensor([h] * batch_size)\n",
    "    if isinstance(w, int):\n",
    "        w = torch.tensor([w] * batch_size)\n",
    "\n",
    "    # Create the grid for each batch (meshgrid for each image)\n",
    "    Y, X = torch.meshgrid([torch.arange(h.max()), torch.arange(w.max())], indexing='ij')\n",
    "    \n",
    "    # Expand X and Y to match batch size and grid size\n",
    "    X = X.unsqueeze(0).expand(batch_size, -1, -1).float().to(device=centers.device)\n",
    "    Y = Y.unsqueeze(0).expand(batch_size, -1, -1).float().to(device=centers.device)\n",
    "\n",
    "    # Calculate the distance of each point in the grid from the center of the circle\n",
    "    dist_from_center = torch.sqrt((X - centers[:, 1].unsqueeze(1).unsqueeze(2)) ** 2 + \n",
    "                                  (Y - centers[:, 0].unsqueeze(1).unsqueeze(2)) ** 2)\n",
    "\n",
    "    # Create a mask for each batch based on the radius\n",
    "    masks = dist_from_center <= radii.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architecture definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftHebbLinear(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            out_channels: int,\n",
    "            t_invert: float = 12, # Invert of the temperature for softmax\n",
    "            # Lateral Support params\n",
    "            n_lateral_neighbors=0, # 0 - turn off lateral support\n",
    "            lateral_period=None, # None - turn off 2D\n",
    "            lateral_gain_coef=0, # 0 - no effect\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        This is the implementation of Linear layer trained with SoftHebb method.\n",
    "        The code is adapted and augmented from https://github.com/NeuromorphicComputing/SoftHebb/tree/main\n",
    "        \"\"\"\n",
    "        super(SoftHebbLinear, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        weight_range = 14 / math.sqrt(in_channels)\n",
    "        self.weight = nn.Parameter(weight_range * torch.randn((out_channels, in_channels)))\n",
    "        self.t_invert = torch.tensor(t_invert)\n",
    "        # Lateral Support params\n",
    "        self.n_lateral_neighbors = n_lateral_neighbors\n",
    "        self.lateral_period = lateral_period\n",
    "        self.lateral_gain_coef = lateral_gain_coef\n",
    "\n",
    "    def forward(self, x):\n",
    "        weighted_input = F.linear(x, self.weight, None)\n",
    "\n",
    "        if self.training:\n",
    "            # Post-synaptic activation, for plastic update, is weighted input passed through a softmax.\n",
    "            # Non-winning neurons (those not with the highest activation) receive the negated post-synaptic activation.\n",
    "            batch_size, out_channels = weighted_input.shape\n",
    "            flat_weighted_inputs = weighted_input\n",
    "            # Compute the winner neuron for each batch element\n",
    "            flat_softwta_activs = torch.softmax(self.t_invert * flat_weighted_inputs, dim=1)\n",
    "            flat_softwta_activs = - flat_softwta_activs  # Turn all postsynaptic activations into anti-Hebbian\n",
    "            win_neurons = torch.argmax(flat_weighted_inputs, dim=1)  # winning neuron for each pixel in each input\n",
    "            competing_idx = torch.arange(flat_weighted_inputs.size(0))  # indices of all pixel-input elements\n",
    "            # Turn winner neurons' activations back to hebbian (pqlet - i.e. make positive, not anti-hebbian term)\n",
    "            flat_softwta_activs[:, win_neurons] = - flat_softwta_activs[:, win_neurons]\n",
    "\n",
    "            # Lateral Support\n",
    "            if self.n_lateral_neighbors > 0:\n",
    "                # 1D case\n",
    "                if self.lateral_period is None:\n",
    "                    # Vectorize the generation of indices for lateral neighbors\n",
    "                    # Range of neighbor offsets\n",
    "                    neighbors_range = torch.hstack(\n",
    "                        (torch.arange(-self.n_lateral_neighbors, 0, device=win_neurons.device),\n",
    "                        torch.arange(1, self.n_lateral_neighbors + 1, device=win_neurons.device))\n",
    "                    )\n",
    "                    # Create the neighbors for each neuron by adding the offsets\n",
    "                    ids_support = win_neurons.unsqueeze(1) + neighbors_range.unsqueeze(0)\n",
    "                    # Clip the resulting indices to be within valid range\n",
    "                    ids_support = torch.clip(ids_support, min=0, max=flat_softwta_activs.size(0) - 1).T\n",
    "                    # Set values\n",
    "                    flat_softwta_activs[:, ids_support] = \\\n",
    "                        + flat_softwta_activs[:, ids_support] \\\n",
    "                        - self.lateral_gain_coef * flat_softwta_activs[competing_idx, win_neurons].unsqueeze(1)\n",
    "                # 2D case\n",
    "                else:\n",
    "                    assert type(self.lateral_period)==int\n",
    "                    # Assume win_neurons are 1D indices of neurons, and n_lateral_period is the number of columns (width of 2D grid)\n",
    "                    n_rows = flat_softwta_activs.size(1) // self.lateral_period  # Total rows in the 2D grid\n",
    "                    # Convert winning neurons' 1D id to 2D\n",
    "                    centers_from_win = torch.stack((\n",
    "                        win_neurons.divide(self.lateral_period, rounding_mode='floor'),\n",
    "                        win_neurons.remainder(self.lateral_period),\n",
    "                    )).T\n",
    "                    # Get circular 2D masks - boolean only by now\n",
    "                    radii = self.n_lateral_neighbors * torch.ones(len(win_neurons), device=win_neurons.device)\n",
    "                    mask_lsupp = create_circular_mask_batched(n_rows, self.lateral_period, \n",
    "                                                                centers=centers_from_win, \n",
    "                                                                radii=radii\n",
    "                                                                )\n",
    "                    # Apply masks\n",
    "                    flat_softwta_activs = \\\n",
    "                        flat_softwta_activs \\\n",
    "                        - self.lateral_gain_coef * flat_softwta_activs[competing_idx, win_neurons].unsqueeze(1) * mask_lsupp.flatten(1)\n",
    "\n",
    "            softwta_activs = flat_softwta_activs\n",
    "            # ===== compute plastic update Î”w = y*(x - u*w) = y*x - (y*u)*w =======================================\n",
    "            yx = torch.matmul(softwta_activs.T, x)\n",
    "\n",
    "            yu = torch.multiply(softwta_activs, weighted_input)\n",
    "            yu = torch.sum(yu.t(), dim=1).unsqueeze(1)\n",
    "            \n",
    "            delta_weight = yx - yu.view(-1, 1,) * self.weight\n",
    "            delta_weight.div_(torch.abs(delta_weight).amax() + 1e-30)  # Scale [min/max , 1]\n",
    "            self.weight.grad = delta_weight  # store in grad to be used with common optimizers\n",
    "\n",
    "        return weighted_input\n",
    "\n",
    "\n",
    "class ModelSoftHebb(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, t_invert, layer_kwargs=None):\n",
    "        super(ModelSoftHebb, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        if layer_kwargs is None:\n",
    "            layer_kwargs = {}\n",
    "        self.layer_kwargs = layer_kwargs \n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(in_channels, affine=False)\n",
    "        self.ff1 = SoftHebbLinear(\n",
    "            in_channels=in_channels, \n",
    "            out_channels=out_channels, \n",
    "            t_invert=t_invert,\n",
    "            **self.layer_kwargs\n",
    "        ) \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.ff1(self.bn1(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization definitions - Scheduler, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorLRSGD(optim.SGD):\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step, using a non-scalar (tensor) learning rate.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum = group['momentum']\n",
    "            dampening = group['dampening']\n",
    "            nesterov = group['nesterov']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad\n",
    "                if weight_decay != 0:\n",
    "                    d_p = d_p.add(p, alpha=weight_decay)\n",
    "                if momentum != 0:\n",
    "                    param_state = self.state[p]\n",
    "                    if 'momentum_buffer' not in param_state:\n",
    "                        buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                    else:\n",
    "                        buf = param_state['momentum_buffer']\n",
    "                        buf.mul_(momentum).add_(d_p, alpha=1 - dampening)\n",
    "                    if nesterov:\n",
    "                        d_p = d_p.add(buf, alpha=momentum)\n",
    "                    else:\n",
    "                        d_p = buf\n",
    "\n",
    "                p.add_(-group['lr'] * d_p)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset definition - CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "datadir = Path(\"../\")\n",
    "\n",
    "# Load DataFrames with embedding ids and texts\n",
    "df_acts_filename = datadir / \"tinystories1M-TinyStories1_gpt2token-acts_id.csv\"\n",
    "df_texts_filename = datadir / \"tinystories1M-TinyStories1_gpt2token-texts.csv\"\n",
    "df_acts_tokens = pd.read_csv(df_acts_filename)\n",
    "df_texts= pd.read_csv(df_texts_filename)\n",
    "\n",
    "# Load embedding matrix\n",
    "with open(datadir / 'emb_matrix.npy', 'rb') as f:\n",
    "    embs_flat = np.load(f)\n",
    "    \n",
    "unsup_trainloader = DataLoader(embs_flat, batch_size=64, pin_memory=True, num_workers=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare model, optimizers, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed training \n",
    "seed_init_fn(42)\n",
    "\n",
    "# Layer kwargs\n",
    "layer_kwargs_2D = {\n",
    "    'n_lateral_neighbors':1,\n",
    "    'lateral_period':60,\n",
    "    'lateral_gain_coef':-0.3,\n",
    "}\n",
    "# Model \n",
    "model = ModelSoftHebb(\n",
    "    in_channels=64, \n",
    "    out_channels=60*30, \n",
    "    t_invert=100,\n",
    "    layer_kwargs=layer_kwargs_2D,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "unsup_optimizer = TensorLRSGD([\n",
    "    # Basically just SGD despite the complicated class, becaouse of just 'lr' param\n",
    "    {\"params\": model.ff1.parameters(), \"lr\": -0.2, },  # SGD does descent, but we assign the positive updates, so set lr to negative\n",
    "], lr=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20-36-12| Unsupervised Training - 1 epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b569c576e67949a69c67b44b6951ad25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10666 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20-37-08| End of Unsupervised Training - 1 epoch\n"
     ]
    }
   ],
   "source": [
    "n_epochs_unsup=1\n",
    "# Unsupervised training with SoftHebb\n",
    "print(f'{datetime.datetime.now().strftime(f\"%H-%M-%S\")}| Unsupervised Training - {n_epochs_unsup} epoch')\n",
    "\n",
    "dd_ker_vals = {'ff1':[]}\n",
    "\n",
    "for epoch in range(n_epochs_unsup):\n",
    "    running_loss = 0.0\n",
    "    for step, data in tqdm(enumerate(unsup_trainloader, 0), total=len(unsup_trainloader)):\n",
    "        global_step = (step+1 + epoch*len(unsup_trainloader))\n",
    "        \n",
    "        inputs = data\n",
    "        inputs = inputs.flatten(1)\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        unsup_optimizer.zero_grad()\n",
    "\n",
    "        # forward + update computation\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "\n",
    "        # optimize\n",
    "        unsup_optimizer.step()\n",
    "                    \n",
    "        # save kernels for plots\n",
    "        ker_vals = deepcopy(model.ff1.weight.cpu().detach())\n",
    "        dd_ker_vals['ff1'].append(ker_vals)\n",
    "\n",
    "print(f'{datetime.datetime.now().strftime(f\"%H-%M-%S\")}| End of Unsupervised Training - {n_epochs_unsup} epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View top tokents to some neuron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb_id</th>\n",
       "      <th>token_str</th>\n",
       "      <th>seq_id</th>\n",
       "      <th>feat</th>\n",
       "      <th>sort_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>607536</th>\n",
       "      <td>607536</td>\n",
       "      <td>dad</td>\n",
       "      <td>2782</td>\n",
       "      <td>0.203379</td>\n",
       "      <td>0.101648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73537</th>\n",
       "      <td>73537</td>\n",
       "      <td>Jill</td>\n",
       "      <td>367</td>\n",
       "      <td>0.195074</td>\n",
       "      <td>0.107867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148784</th>\n",
       "      <td>148784</td>\n",
       "      <td>dad</td>\n",
       "      <td>737</td>\n",
       "      <td>0.192554</td>\n",
       "      <td>0.089043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463881</th>\n",
       "      <td>463881</td>\n",
       "      <td>dad</td>\n",
       "      <td>2166</td>\n",
       "      <td>0.192051</td>\n",
       "      <td>0.094174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674053</th>\n",
       "      <td>674053</td>\n",
       "      <td>children</td>\n",
       "      <td>3153</td>\n",
       "      <td>0.191810</td>\n",
       "      <td>0.144837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18562</th>\n",
       "      <td>18562</td>\n",
       "      <td>parents</td>\n",
       "      <td>99</td>\n",
       "      <td>0.191360</td>\n",
       "      <td>0.109007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577363</th>\n",
       "      <td>577363</td>\n",
       "      <td>friends</td>\n",
       "      <td>2638</td>\n",
       "      <td>0.191189</td>\n",
       "      <td>0.116324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271151</th>\n",
       "      <td>271151</td>\n",
       "      <td>dad</td>\n",
       "      <td>1212</td>\n",
       "      <td>0.190664</td>\n",
       "      <td>0.086794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12402</th>\n",
       "      <td>12402</td>\n",
       "      <td>daddy</td>\n",
       "      <td>69</td>\n",
       "      <td>0.189825</td>\n",
       "      <td>0.093433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162029</th>\n",
       "      <td>162029</td>\n",
       "      <td>pa</td>\n",
       "      <td>768</td>\n",
       "      <td>0.188926</td>\n",
       "      <td>0.083726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        emb_id  token_str  seq_id      feat  sort_value\n",
       "607536  607536        dad    2782  0.203379    0.101648\n",
       "73537    73537       Jill     367  0.195074    0.107867\n",
       "148784  148784        dad     737  0.192554    0.089043\n",
       "463881  463881        dad    2166  0.192051    0.094174\n",
       "674053  674053   children    3153  0.191810    0.144837\n",
       "18562    18562    parents      99  0.191360    0.109007\n",
       "577363  577363    friends    2638  0.191189    0.116324\n",
       "271151  271151        dad    1212  0.190664    0.086794\n",
       "12402    12402      daddy      69  0.189825    0.093433\n",
       "162029  162029         pa     768  0.188926    0.083726"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic = 15 + 60\n",
    "name = f\"feat\"\n",
    "df_acts_tokens[name] = (embs_flat @ model.ff1.weight[ic].detach().cpu().numpy())\n",
    "df_acts_tokens.sort_values(by=name, ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot tokents weights are specific to in the 2D grid of output neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31de126173344bac886bf402b45d70e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pqlet\\AppData\\Local\\Temp\\ipykernel_20024\\506872894.py:7: FutureWarning:\n",
      "\n",
      "The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save top 3 unique tokens for every output neuron\n",
    "joint_text = []\n",
    "for ic in tqdm(range((model.ff1.weight.shape[0]))):\n",
    "    activ_ic = (embs_flat @ model.ff1.weight[ic].detach().cpu().numpy())\n",
    "    df_acts_tokens['sort_value'] = activ_ic\n",
    "    top_token_str = df_acts_tokens.sort_values(by='sort_value', ascending=False).token_str\n",
    "    joint_text.append('<br>'.join(top_token_str[:3].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "x=%{x}<br>y=%{y}<br>text=%{text}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "text": [
          " make",
          " making<br> make",
          " be",
          " been<br> be",
          " be",
          " getting<br> become",
          " got",
          " the",
          " the",
          " the",
          " the",
          " the",
          " the",
          " the",
          " daughter<br> Jack<br> George",
          " Jill<br>pa<br> dad",
          "pa<br> captain",
          " dad<br>pa",
          " and",
          " and",
          " and",
          " and",
          " also<br> and",
          " he",
          " Sam<br> Ben",
          " Sam<br> Ben",
          " Sam<br> Mia<br>ill",
          " Sam<br>Tom<br> Ben",
          " chip<br> little<br> elderly",
          " elderly<br> chip<br> bald",
          " new<br> shiny<br> pale",
          "oc<br> rearr<br> Fl",
          " grateful<br>gr<br> kind",
          " been<br> was",
          " We<br>We",
          " on<br> in",
          " right<br>gr",
          " you<br> You",
          "We<br> we<br> We",
          "We",
          " I",
          " try",
          " managed<br> began<br> started",
          " carefully<br> quickly",
          " maze<br> broom<br> elevator",
          " tent<br> reef<br> maze",
          " pit<br> reef",
          " garage",
          " valley<br> beach",
          " valley<br> cave<br> river",
          " before",
          " if",
          " one",
          " one",
          " something",
          " something",
          "\"",
          " day",
          " day<br> time",
          " day",
          " make<br> making",
          " make",
          " and",
          " be<br> and",
          " getting<br> be",
          " getting",
          " getting",
          " d<br> breat<br> pl",
          " the",
          " the",
          " the",
          " the",
          " the",
          " the",
          " the",
          " dad<br> Jill",
          " dad<br> Mom<br> mummy",
          " mom<br> Mom",
          " mum<br> mom<br> Mom",
          " and",
          " and",
          " and",
          " she",
          " she",
          " she",
          " Sam<br> Ben<br> Sara",
          " Sam<br>Tom<br>ill",
          " she<br> he",
          " and<br> She",
          " little",
          " bald<br> le",
          " Fl<br> L<br>oc",
          "gr<br> un<br> cho",
          "gr<br> been",
          "That<br> That",
          " team<br> job<br> one",
          " you",
          " you",
          " I<br>We",
          " I",
          "I<br> I",
          " tried",
          " started",
          " began<br> started",
          " maze<br> mountain",
          " maze<br> valley",
          " curtain<br> basement<br> garage",
          " room<br> table",
          " fridge<br> garage<br> freezer",
          " barrel<br> hook",
          "\n",
          " moral",
          " next",
          " oct<br> m",
          " something",
          "\"<br> \"",
          " \"",
          "\"<br> \"",
          " day",
          " day",
          "Ã¢",
          "oc<br>Ap<br>Ã¢",
          " and",
          " and",
          " and",
          " decided<br> was",
          " get",
          " se<br> kay",
          " se<br> z<br> d",
          " mer<br> w",
          " the<br> ugly<br> unusual",
          " the",
          " the",
          " the",
          " the",
          " Dad<br> mummy<br> mom",
          " dad<br> Dad",
          " Mom",
          " dad<br> Mom<br>ma",
          " mom<br>Dad<br>Daddy",
          " and",
          " and",
          " she",
          " and<br> she",
          " and",
          " and",
          " and",
          " and",
          " and",
          " and",
          " bald<br> team<br> polar",
          " Sue<br>Tom<br> Bob",
          " mom",
          " Lily<br> Will<br> bear",
          "That<br>Is<br>three",
          " veil",
          " turns<br> wind",
          " you",
          " we",
          " I",
          " I<br>I",
          ",",
          " began<br> started",
          " hut<br> valley<br> chim",
          " valley<br> tent",
          " chim",
          " freezer<br> pit<br> roof",
          " box<br> drawer<br> cabinet",
          " fridge<br> cabinet<br> closet",
          " jar<br> tray",
          "\n",
          "\n",
          "â‚¬<br> Fl",
          "â‚¬",
          "â‚¬",
          " \"",
          " \"",
          "\"",
          " day",
          " day",
          "Ã¢",
          "Ã¢<br>oc",
          "oc<br>Ap<br> se",
          " and",
          " and",
          " decided",
          " returned",
          " gl",
          " se",
          "oc<br> y",
          " ugly<br> unusual",
          " the",
          " the",
          " the",
          " the",
          " the",
          " Mom",
          " Mom",
          " Mom",
          " Sam<br> boys",
          " Sam",
          " things",
          " and",
          " and",
          " and",
          " and",
          " or<br> and",
          " or<br> and",
          " and",
          " dog<br> bear",
          " Molly<br> Jack<br> Jim",
          " Ben<br> Molly",
          "Anna<br> Lily",
          " them",
          " them<br> Lily<br> ours",
          " mess",
          " mess<br> forgive",
          " like",
          " I",
          " I",
          ",",
          ",",
          " hut<br> backyard",
          " tray<br> jar",
          " valley<br> tent<br> chamber",
          " chim",
          " chim",
          " stone<br> chain",
          " boots<br> bus<br> belt",
          "\n",
          "\n",
          "\n",
          "\n<br>â‚¬",
          "â‚¬",
          " \"<br> ''",
          "\"",
          " \"",
          "\"",
          " day",
          " day",
          "Ap<br> Panc",
          " resc<br>Ap<br> sw",
          " resc<br>Ap<br> spe",
          "Ap<br>R<br>oc",
          "Let",
          " decided",
          " living",
          " pand<br> ostr",
          " ostr<br> pand<br> re",
          "oc",
          " sw<br> m<br> ugly",
          " the",
          " the",
          " the",
          "The<br> the",
          " the",
          " the",
          " Mom",
          "Dad",
          " dad<br>Dad<br> Dad",
          " idea",
          " ones<br> things",
          " and",
          " and",
          " and",
          " and",
          " and",
          " or",
          " doll<br> bear",
          " elf<br> beetle<br> dragon",
          " dragon<br> veil<br> hat",
          " doctor<br> man<br> dragon",
          "ears",
          " them<br> it",
          " you",
          " understand<br> mess",
          " loved",
          " loved",
          " this<br>This",
          ",",
          ",",
          ",",
          " hut<br> cabinet",
          " tray<br> chim",
          " sand<br> tent<br> pit",
          " chim<br> pit",
          " pit<br> mountain<br> windows",
          " freezer<br> stone<br> blanket",
          " maze<br>\n",
          "\n",
          "\n",
          "\n",
          "\n",
          "\n",
          " \"",
          " \"",
          " \"",
          ".",
          ".",
          ".",
          "â„¢",
          " Ch<br> L<br> Ab",
          " resc<br>Ap<br>oc",
          "Ap<br> resc<br> ï¿½",
          "Let",
          " in",
          " in",
          " living",
          " ostr<br> er<br>-",
          "oc",
          " pr<br> sw<br> le",
          " the",
          " the",
          " the<br> large",
          " the",
          " the",
          " the",
          " the",
          "Mom<br> Mom",
          " learn",
          " teaches<br> forgot",
          " or<br> and",
          " and",
          " and",
          " and",
          " and",
          " and",
          " to",
          " bird",
          " helicopter<br> hat",
          " veil<br> doll",
          " veil<br> soup",
          " sugar<br> lemon",
          "ears",
          " you",
          " that",
          " that",
          " this<br>This<br> This",
          " This<br>This",
          ",",
          ",",
          " from<br>,",
          " tray<br> sack<br> freezer",
          " sand<br> log<br> lumber",
          " snow<br> soap",
          " pit<br> valley<br> sand",
          " sky",
          " sky",
          "\n",
          "\n",
          "\n",
          "\n",
          "\n",
          "\n",
          "\n",
          ".",
          ".",
          ".",
          ".",
          ".",
          " L<br>J",
          " L<br>J",
          "R<br> Ab",
          "R<br> oct<br> P",
          " in",
          " in",
          " in",
          " in",
          " kay<br> ch<br>ind",
          " pr<br> er",
          " two<br> chip",
          " big<br> fake<br> new",
          " a",
          " the",
          " the",
          " the",
          " the",
          " the",
          " the",
          " know",
          " understand<br> know",
          "e<br>oc<br> J",
          " and",
          " and",
          " to",
          " to",
          " to",
          " to",
          "Uh<br>arl<br> bird",
          " wind",
          " coconut<br> diamond<br> rocket",
          " cake<br> maze<br> rainbow",
          " popcorn<br> hats<br> screw",
          "ears<br> coin<br> gems",
          " doll<br> coin<br> comet",
          " that",
          " that",
          " that",
          "That<br> That",
          ".",
          ".",
          " from",
          " all",
          " all",
          " valley<br> pit<br> sand",
          " valley<br> mountain<br> creek",
          " valley<br> tunnel<br> chim",
          " was",
          "\n",
          "\n",
          "\n",
          "\n",
          "\n",
          "\n",
          ".\"<br>\".<br>.",
          ".",
          ".",
          ".",
          ".",
          ".",
          " P<br> M",
          " M",
          " Ab",
          " Ab",
          " Ab",
          " through<br> in",
          " on",
          " of",
          " more",
          " many",
          " a",
          " a",
          " a",
          " the<br> large",
          " the",
          " the",
          " the",
          " the",
          " Jill<br> Sue<br> Tim",
          " Jill<br> Sam<br> Maria",
          "lon<br> Jill<br> Sam",
          "\n",
          " to",
          " to",
          " to",
          " to",
          " to",
          " to",
          " to",
          " wind",
          " flame<br> shampoo<br> powder",
          " banana<br> bone<br> branch",
          "anda",
          " doll<br> comet",
          " friends",
          " that",
          " little",
          " that",
          " a",
          ".",
          ".",
          ".",
          " all",
          " all",
          " out",
          " swamp<br> woods<br> valley",
          "'m<br> am",
          " was",
          " was",
          " are",
          "\n",
          "\n",
          "\n",
          " it",
          ".",
          ".",
          ".",
          ".",
          ".",
          ".",
          " Sam<br> Molly<br> Ben",
          " Ab",
          " Ab",
          " Ab",
          " over",
          " over",
          " into<br> of<br> on",
          " of",
          " of",
          " of",
          " a",
          " a",
          " a",
          " red",
          " black<br> large<br> shiny",
          " incredible<br> powerful<br> beautiful",
          " smile",
          "nine",
          " Ben<br> Sara",
          " Jill<br> Lily",
          "\n",
          "\n",
          " to",
          " to",
          " to",
          " to",
          " to",
          " to",
          " crow<br> bull<br> man",
          " flame<br> bulb<br> candle",
          " flag<br> missile<br> veil",
          " crane<br>anda",
          " raven<br> crow<br>ebra",
          " angel<br> raven<br> lion",
          " Brown<br>dad<br>pa",
          " young",
          " young",
          " a",
          " a",
          "P<br>Lu",
          ".",
          ".",
          " all",
          " all",
          " out",
          " out",
          " was",
          " was",
          " were<br> was",
          " is",
          " It",
          " It",
          " it",
          " it",
          " it",
          ".",
          ".",
          ".",
          ".",
          ".",
          " Winn<br> Bob<br> Joe",
          " Winn<br> Ab",
          " Ab",
          "Charlie<br>Lu",
          "dy<br>agg",
          " build",
          " sell",
          " of",
          " of",
          " at",
          " a",
          " a",
          " big<br> a",
          " green<br> red",
          " small<br> green",
          " bald<br>year<br> thin",
          " smile",
          "\"",
          "\"",
          "\n",
          "\n",
          "\n",
          "\n",
          "\n",
          " to",
          " to",
          ",\"",
          ",\"",
          " man",
          " crow<br> man<br>anda",
          " robot<br> crane<br> dolphin",
          " bear<br>ule<br> pig",
          " crow<br>ule<br> dolphin",
          "itt<br> ot<br> ostr",
          " ostr<br> rh<br> Fl",
          " old",
          "ubby<br> bald<br> playful",
          " bald<br> cute<br> compassionate",
          " a",
          "L",
          ".",
          ".",
          " on",
          " on",
          " off",
          " out<br> up",
          "Can<br> Can",
          " Or<br> wished",
          " would<br> get<br> was",
          " his<br> her<br> their",
          " beautiful<br> special<br>It",
          " It",
          " It",
          " It",
          " It<br> it",
          ".",
          ".",
          ".<br>!",
          "!<br>.",
          "!<br>.",
          " Judy<br>dy",
          " Judy<br> Winn<br> Sammy",
          " Judy<br> Maria<br> Joe",
          " Jill",
          " sail<br> costumes",
          " fold<br> print<br> sail",
          " feed<br> feeding",
          " at<br> of",
          " of",
          " of",
          " a",
          " a",
          " a",
          " big",
          ",",
          ",",
          "\"",
          "\"",
          "\"",
          "\"",
          "\n",
          "\n",
          "\n",
          "\n",
          "\n",
          ",\"",
          ",\"",
          ",\"",
          " boy<br> Sam",
          " boy<br> kid",
          "avin<br> kid",
          " prince",
          " queen<br> prince<br> princess",
          "ebra<br> hipp",
          " oct<br> ot",
          " bald<br> er<br>oc",
          " tall<br> thin<br> strong",
          " healthy<br> tough<br> smart",
          " sw<br>oc",
          "J",
          ".",
          ".",
          " under",
          " on",
          " up",
          " up",
          "Hey",
          "Thank",
          " happy<br> careful",
          " real<br> angry",
          " long<br> good<br> great",
          " that",
          " that",
          " it<br> It",
          " it",
          " it<br> you",
          "!<br>.",
          "!<br>.",
          "!<br>.",
          "!",
          " Ab",
          " Judy<br> Ang<br> Winn",
          " brothers<br> twins",
          " maze<br> stadium<br> trucks",
          " surf<br> chess",
          " sweep<br> stir",
          " chew<br> bury",
          " open",
          " for",
          " for",
          " a",
          " a",
          " a",
          " a",
          ",",
          ",",
          "\"<br> \"",
          "\"",
          "\"",
          "\n",
          "\n",
          "\n",
          "\n",
          "\n",
          "'.\"<br>!\"<br>!'\"",
          "!\"<br>.\"",
          ",\"",
          ",\"<br>?\"",
          "andy<br> Sam",
          "ila<br>addy",
          " Maria<br>Ted",
          "Ted<br>innie",
          " crab<br> prince<br> bull",
          " crab<br> fox<br>inger",
          " oct",
          " big",
          " big",
          " big<br> smooth",
          " different<br> vibrant",
          "oc<br> purple",
          "oc<br>year",
          " on",
          " on",
          " on",
          " up",
          " up",
          "Come",
          "oc<br> your",
          " their<br> real",
          " new<br> their<br> your",
          " messy<br> peaceful<br> soft",
          " messy<br>elly<br> purple",
          " who",
          " ago<br> There",
          " you<br> we",
          " you",
          " you",
          "!",
          ".",
          " to",
          " Ang<br> Ab",
          "anut<br> Winn<br> Ang",
          " ot<br> ostr",
          " razor<br> spike<br> violin",
          "itten",
          " surf<br> chess<br> cardboard",
          " bury<br> throw",
          " climb",
          " for",
          " for",
          " for",
          " for",
          " would<br> could",
          " can",
          " can",
          ",",
          " \"",
          " \"<br>\"",
          "\"",
          "\n",
          "\n",
          "\n",
          "\n",
          "\n",
          "?\"",
          "!\"<br>?\"",
          ",\"",
          "andy<br>ill",
          " Sam",
          " Timothy<br> Sam<br> Jen",
          " Maria<br> Larry<br> Jess",
          " Maria<br>Charlie<br>Andy",
          ".",
          ".",
          ".",
          ".<br>!",
          " vast<br> tall",
          " deep<br> black<br> smooth",
          " unusual<br> ordinary<br> shiny",
          "oc",
          " good",
          " big",
          " through<br> in",
          " through<br> in",
          " was",
          " was",
          " their<br> your",
          " your",
          " their",
          " their<br> your",
          " their<br> your",
          " our",
          "Lu<br> who",
          " we<br> Who<br> I",
          " you",
          " you",
          " to",
          "'s",
          " to",
          " to<br> must",
          "oto<br>ira<br>anut",
          " cricket<br> bug<br>ebra",
          " crab<br> raven<br> dragon",
          " missile<br> rocket<br> raft",
          " razor<br> spike<br> er",
          " boun<br> gem",
          " sail",
          " sail<br> soaring",
          " sail",
          " for",
          " for",
          " can",
          " would",
          " can",
          " can",
          " can",
          " \"",
          ",",
          "\"<br> \"",
          "\n",
          "\n",
          "\n",
          "\n",
          "\n",
          " come",
          "?\"",
          "Charlie<br>bie<br> Jim",
          "lon<br>avin",
          "lon<br>ola",
          "ill<br>lon<br>bie",
          " Maria<br> Sally<br>Ted",
          "Charlie<br>Lu",
          ".",
          ".",
          "!<br>.",
          ".<br>!",
          ".",
          " sw<br> black<br> smooth",
          " black<br> purple<br> red",
          " black<br> red",
          " shiny<br> black<br> smooth",
          " an",
          " an",
          " was",
          " was",
          " was",
          " was",
          " upon",
          " their<br> your",
          " your<br> their",
          " your<br> their",
          " your<br> our<br> their",
          "'s<br> your<br> our",
          " You",
          " time",
          " to",
          " to",
          " to",
          " to",
          " to",
          " kids",
          " mice<br> bus<br> puck",
          " avocado<br> squash<br> missile",
          " stone<br> coin<br> gem",
          " bow<br> ribbon<br> necklace",
          " needles<br> stones",
          " ramps<br> rocks<br>ears",
          " soaring<br> sail",
          " stopped",
          " stopped<br> why",
          "Where<br> Where",
          "Where",
          " can<br> could",
          " could<br> can",
          " can",
          " could",
          ",",
          ",",
          ",",
          "\n",
          "\n",
          "\n",
          "Once",
          "Once",
          " came",
          "lon<br>avin",
          "avin<br>lon",
          "avin",
          "lon<br>avin",
          "ill<br>lon",
          "ill",
          ".",
          ".",
          ".<br>!",
          ".<br>!",
          ".",
          ".",
          ".",
          " shiny<br> ugly<br> pale",
          " angry<br> real<br> rough",
          " wise<br> black<br> mighty",
          " an",
          " was",
          " were",
          " was",
          " was",
          " upon",
          " upon",
          " upon",
          " your",
          " your<br> my",
          " my<br> your",
          " her<br> his",
          " time",
          " time",
          " time",
          " to",
          " to",
          " to",
          " to",
          " trucks<br> rockets<br> shells",
          " oy<br>itt",
          " fork<br> magnet<br> avocado",
          " jelly<br> caul<br> oy",
          " stones<br> shells<br> gems",
          " shells<br> stones<br> rocks",
          " nests<br> shells<br> rocks",
          " cos<br> sit",
          " sitting",
          " how",
          "what<br>What",
          " why<br> How",
          " How<br>How",
          " can",
          " can",
          " could",
          ",",
          ",",
          ",",
          "\n",
          "\n",
          "Once",
          "Once",
          "Once",
          " came",
          " blinked<br> nodded",
          "avin<br>lon",
          "avin<br>lon",
          "Tim",
          " Sam<br>Tim",
          "Charlie<br>Ted",
          ".",
          ".",
          ".",
          ".",
          ".",
          ".",
          " had",
          " have",
          " smooth<br> messy<br> shiny",
          " mer<br> mean<br> naughty",
          " cute",
          " was",
          " lived<br> was",
          " lived",
          " liked",
          " upon",
          " upon",
          " upon",
          " my<br> his",
          " your<br> my",
          " my",
          " his",
          "Once",
          " time",
          " time",
          " to",
          " to",
          " to",
          " to"
         ],
         "textposition": "top center",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59
         ],
         "xaxis": "x",
         "y": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "font": {
         "size": 10
        },
        "height": 1200,
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "paper_bgcolor": "white",
        "plot_bgcolor": "white",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "2D Grid of neurons and their associate tokens"
        },
        "width": 3000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "showgrid": false,
         "showticklabels": false,
         "title": {
          "text": "x"
         },
         "zeroline": false
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "showgrid": false,
         "showticklabels": false,
         "title": {
          "text": "y"
         },
         "zeroline": false
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# layer_kwargs_2D['lateral_period']\n",
    "# fig = px.scatter(x=np.arange(0,60,1).repeat(10), y=np.tile(np.arange(0,10), 60), text=joint_text[:600])\n",
    "x_rows = 16\n",
    "fig = px.scatter(\n",
    "    x=np.tile(np.arange(0,60), x_rows), \n",
    "    y=np.arange(0,x_rows,1).repeat(60), \n",
    "    text=joint_text[:60*x_rows]\n",
    ")\n",
    "\n",
    "fig.update_traces(textposition='top center')\n",
    "\n",
    "fig.update_layout(\n",
    "    height=1200,\n",
    "    width=3000,\n",
    "    plot_bgcolor='white',  # Background color of the plot area\n",
    "    paper_bgcolor='white', # Background color of the area outside the plot\n",
    "    title_text='2D Grid of neurons and their associate tokens',\n",
    "    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "    # xaxis_range=[11, 31],\n",
    "    # yaxis_range=[-0.5, x_rows+1],\n",
    "    font=dict(size=10) \n",
    ")\n",
    "fig.show()\n",
    "\n",
    "from pathlib import Path\n",
    "dir = Path('image-pres')\n",
    "dir.mkdir(exist_ok=True)\n",
    "fig.write_image(f'{dir}/2D-neuron-map-words.png', format='png', scale=6, width=3000, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dls_nlp",
   "language": "python",
   "name": "dls_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
